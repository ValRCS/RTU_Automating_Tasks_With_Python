{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde3df92",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using HTTP or through a browser.\n",
    "\n",
    "![Robots](https://upload.wikimedia.org/wikipedia/commons/6/63/Web_Robots_Logo.png)\n",
    "\n",
    "## How the scraping process works\n",
    "* request the URL (uniform resource locator / web address)\n",
    "* parse the response text\n",
    "* find required HTML elements and their content\n",
    "* save the results in the format you need\n",
    "\n",
    "## Scraping responsibly\n",
    "* Check for other ways of accessing data (JSON API, CSV download, etc.).\n",
    "* Play nice: do not overload sites with requests.\n",
    "* Access publicly available data only.\n",
    "* Respect robots.txt.\n",
    "* Remember the difference between scraping something and how you use it (publishing, monetizing, etc.).\n",
    "\n",
    "### Legality note\n",
    "As of Sep 2019 the direction is that it should be legal to scrape publicly available data provided you do not disturb the normal operation of the website. See https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70b8c8",
   "metadata": {},
   "source": [
    "## HTML and HTTP fundamentals\n",
    "* https://developer.mozilla.org/en-US/docs/Web/HTML\n",
    "* https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML/Getting_started\n",
    "* https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview\n",
    "* Chrome DevTools: https://developers.google.com/web/tools/chrome-devtools/open\n",
    "\n",
    "Understanding both is essential for scraping: HTML tells you which tags and attributes hold the data you want, while HTTP defines how you request pages, handle status codes, respect robots.txt, and throttle or paginate responsibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61ba85",
   "metadata": {},
   "source": [
    "## Scraping tables with pandas\n",
    "Pandas data analysis has basic web scraping capabilities via `pandas.read_html`. It works when a page exposes data in `<table>` elements, automatically returning each table as a dataframe. If the data is not in tabular form, use BeautifulSoup or Scrapy instead (see the separate BeautifulSoup notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"Date: {datetime.now()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15279bd9",
   "metadata": {},
   "source": [
    "## Choose a target URL\n",
    "The example below scrapes flat listings. Notice how query parameters in the URL describe the target area. Adjust the URL to match the data you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f12439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.ss.com/en/real-estate/flats/riga/centre/sell/\"\n",
    "# url = \"https://www.ss.com/en/real-estate/flats/riga/centre/hand_over/\"  # renting\n",
    "url = \"https://www.ss.com/en/real-estate/flats/riga/agenskalns/sell/\"\n",
    "print(f\"URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495c600",
   "metadata": {},
   "source": [
    "## Read HTML tables into dataframes\n",
    "`pandas.read_html` downloads the page, parses all HTML tables, and returns a list. Some sites (like ss.com) use tables for layout, so expect more than one table and pick the one that holds the data you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e954079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_html(url, header=0)\n",
    "print(f\"Found {len(dfs)} tables (type: {type(dfs).__name__})\")\n",
    "\n",
    "if len(dfs) <= 4:\n",
    "    raise ValueError(\"Expected at least 5 tables on the page to access index 4.\")\n",
    "\n",
    "df = dfs[4]  # the 5th table on the page holds the listings\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356ce39",
   "metadata": {},
   "source": [
    "## Inspect the scraped table\n",
    "Always check the shape and a sample of the data before exporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be3801",
   "metadata": {},
   "source": [
    "## Save the results\n",
    "Dataframes can be saved to many formats. Use `index=False` when you do not want the dataframe index written out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f18832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"agenskalns.json\", orient=\"records\", index=False)\n",
    "df.to_csv(\"agenskalns.csv\", index=False)\n",
    "df.to_excel(\"agenskalns.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292816fa",
   "metadata": {},
   "source": [
    "## Combine multiple pages\n",
    "You can scrape additional pages (e.g., pagination) and concatenate them. Here we fetch a second page, grab its listings table, and stack the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://www.ss.com/en/real-estate/flats/riga/agenskalns/sell/page2.html\"\n",
    "print(f\"URL: {url2}\")\n",
    "\n",
    "dfs_page2 = pd.read_html(url2, header=0)\n",
    "if len(dfs_page2) <= 4:\n",
    "    raise ValueError(\"Expected at least 5 tables on the second page to access index 4.\")\n",
    "\n",
    "df_page2 = dfs_page2[4]\n",
    "combined = pd.concat([df, df_page2], ignore_index=True)\n",
    "print(f\"Combined shape: {combined.shape}\")\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c87d61",
   "metadata": {},
   "source": [
    "## Save combined data\n",
    "Export the merged dataframe for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_excel(\"agenskalns_big.xlsx\", index=False)\n",
    "combined.to_csv(\"agenskalns_big.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027b270",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "* Loop through pages until you detect the last one automatically.\n",
    "* Extract detail-page links from each row with BeautifulSoup to enrich the dataset.\n",
    "* Respect the target site's terms of service and throttle requests.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
